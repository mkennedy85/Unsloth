{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 1: Full Fine-Tuning with SmolLM2-135M\n\nThis notebook demonstrates full parameter fine-tuning using Unsloth.ai with a small model (SmolLM2-135M).\n\n## Key Concepts\n- **Full Fine-Tuning**: Updates all model parameters (vs LoRA which only updates adapters)\n- **Model**: SmolLM2-135M - A tiny but capable model perfect for learning\n- **Task**: Instruction following / Chat completion\n- **Dataset**: Alpaca-style instruction dataset\n\n## Video Recording Checklist\n- [ ] Explain what full fine-tuning means\n- [ ] Show model architecture and parameter count\n- [ ] Walk through dataset format\n- [ ] Explain training hyperparameters\n- [ ] Show training progress and metrics\n- [ ] Demonstrate inference before/after\n- [ ] Export to Ollama"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture\n# Install Unsloth and dependencies\n!pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model with Full Fine-Tuning Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 2048  # SmolLM2 can handle up to 2048 tokens\n",
    "dtype = None  # Auto-detect. Use Float16 for Tesla T4, V100, or bfloat16 for Ampere+\n",
    "load_in_4bit = False  # We want full precision for full fine-tuning\n",
    "\n",
    "# Load SmolLM2-135M model\n",
    "# Alternative: \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\" for slightly larger model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-135M-Instruct\",  # 135M parameters - very small!\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure for Full Fine-Tuning (NOT LoRA)\n",
    "\n",
    "**Important**: Setting `use_gradient_checkpointing=\"unsloth\"` with no LoRA modules means full fine-tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for full fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 0,  # No LoRA rank - this means full fine-tuning!\n",
    "    target_modules = None,  # No specific target modules\n",
    "    lora_alpha = 0,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Memory efficient\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "print(\"Model configured for FULL FINE-TUNING\")\n",
    "print(\"All parameters will be updated during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and Prepare Dataset\n",
    "\n",
    "We'll use a small instruction-following dataset. Format:\n",
    "```\n",
    "{\n",
    "  \"instruction\": \"What is the capital of France?\",\n",
    "  \"input\": \"\",\n",
    "  \"output\": \"The capital of France is Paris.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Alpaca dataset (cleaned version with 52k instructions)\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "\n",
    "# Let's look at a few examples\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"\\nFirst example:\")\n",
    "print(dataset[0])\n",
    "print(\"\\nDataset columns:\", dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Chat Template\n",
    "\n",
    "We need to format the data according to SmolLM2's chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmolLM2 uses a simple chat template\n",
    "chat_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # End of sequence token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Combine instruction and input if input exists\n",
    "        text = chat_template.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting to dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Show a formatted example\n",
    "print(\"Formatted example:\")\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure Training Arguments\n",
    "\n",
    "For full fine-tuning, we use smaller learning rates than LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4,  # Batch size per GPU\n",
    "    gradient_accumulation_steps = 4,  # Effective batch size = 4 * 4 = 16\n",
    "    warmup_steps = 100,\n",
    "    num_train_epochs = 1,  # 1 epoch for demo, increase for better results\n",
    "    max_steps = 500,  # Limit steps for faster training\n",
    "    learning_rate = 5e-5,  # Lower LR for full fine-tuning vs 2e-4 for LoRA\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 10,\n",
    "    optim = \"adamw_8bit\",  # Memory efficient optimizer\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs/smollm2_full_finetuned\",\n",
    "    report_to = \"none\",  # Disable wandb/tensorboard for now\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Total steps: {training_args.max_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Trainer and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,  # Can make training 5x faster for short sequences\n",
    "    args = training_args,\n",
    ")\n",
    "\n",
    "print(\"Trainer created. Starting training...\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING IN PROGRESS\")\n",
    "print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Inference\n",
    "\n",
    "Let's test the fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable fast inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Write a Python function to calculate factorial\",\n",
    "    \"Explain machine learning in simple terms\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    formatted_prompt = chat_template.format(prompt, \"\", \"\")\n",
    "    inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"RESPONSE:\\n{response}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full fine-tuned model\n",
    "model.save_pretrained(\"smollm2_full_finetuned\")\n",
    "tokenizer.save_pretrained(\"smollm2_full_finetuned\")\n",
    "\n",
    "print(\"Model saved to: smollm2_full_finetuned/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Export to Different Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GGUF format for llama.cpp\n",
    "model.save_pretrained_gguf(\n",
    "    \"smollm2_full_finetuned_gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method = \"q4_k_m\"  # 4-bit quantization\n",
    ")\n",
    "\n",
    "print(\"Exported to GGUF format!\")\n",
    "\n",
    "# Export to float16 for Ollama\n",
    "model.save_pretrained_gguf(\n",
    "    \"smollm2_full_finetuned_ollama\",\n",
    "    tokenizer,\n",
    "    quantization_method = \"f16\"  # Float16 for Ollama\n",
    ")\n",
    "\n",
    "print(\"Exported to Ollama-compatible format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Upload to HuggingFace (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and fill in your HuggingFace username\n",
    "# model.push_to_hub(\"your_username/smollm2-135m-alpaca-full-finetuned\", token=\"YOUR_HF_TOKEN\")\n",
    "# tokenizer.push_to_hub(\"your_username/smollm2-135m-alpaca-full-finetuned\", token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "print(\"To upload to HuggingFace, uncomment the code above and add your token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### What we accomplished:\n1. Loaded SmolLM2-135M model (135 million parameters)\n2. Configured for **full fine-tuning** (all parameters updated)\n3. Fine-tuned on Alpaca instruction dataset (52k examples)\n4. Tested inference with custom prompts\n5. Exported to multiple formats (GGUF, Ollama)\n\n### Key Differences from LoRA:\n- **Full Fine-Tuning**: Updates ALL model parameters (135M)\n- **LoRA**: Only updates adapter parameters (~1-2M)\n- **Memory**: Full FT requires more VRAM\n- **Speed**: LoRA is faster to train\n- **Quality**: Full FT can achieve better results but risks overfitting\n\n### Next Steps:\n- Compare with LoRA results in Notebook 2\n- Try with larger models (Gemma-3-1B)\n- Experiment with different learning rates\n- Test on domain-specific datasets"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}