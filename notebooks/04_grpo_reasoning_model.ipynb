{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: GRPO Reasoning Model Training\n",
    "\n",
    "This notebook demonstrates **Group Relative Policy Optimization (GRPO)** for training reasoning models similar to OpenAI's o1 or DeepSeek-R1.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What is GRPO?\n",
    "**GRPO** is a reinforcement learning technique that:\n",
    "- Generates multiple solutions for each problem\n",
    "- Uses relative ranking within groups instead of absolute rewards\n",
    "- Trains models to think step-by-step (chain-of-thought)\n",
    "- Improves reasoning capabilities\n",
    "\n",
    "### How GRPO Works:\n",
    "1. **Generate**: Create multiple candidate solutions for each problem\n",
    "2. **Evaluate**: Check which solutions are correct\n",
    "3. **Rank**: Group solutions and rank them relatively\n",
    "4. **Optimize**: Update model to favor better solutions\n",
    "\n",
    "### Why Use GRPO?\n",
    "- Better than standard RLHF for reasoning tasks\n",
    "- More stable than PPO\n",
    "- Encourages exploration of solution space\n",
    "- Natural fit for math/coding problems\n",
    "\n",
    "## Dataset Format\n",
    "\n",
    "GRPO needs problems with verifiable answers:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What is 15% of 80?\",\n",
    "  \"answer\": \"12\",\n",
    "  \"solution\": \"15% = 0.15\\n0.15 × 80 = 12\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Video Recording Checklist\n",
    "- [ ] Explain what reasoning models are (o1, DeepSeek-R1)\n",
    "- [ ] Show how GRPO differs from DPO/ORPO\n",
    "- [ ] Demonstrate multi-solution generation\n",
    "- [ ] Show reward calculation and ranking\n",
    "- [ ] Compare reasoning quality before/after\n",
    "- [ ] Test on various math problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth and dependencies\n",
    "# Use colab-new for Google Colab, cu121-torch230 for Vertex AI Workbench\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchDPOTrainer\n",
    "PatchDPOTrainer()\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "import re\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Base Model\n",
    "\n",
    "For reasoning tasks, we'll use a slightly larger model with good instruction-following capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Using Gemma 2 2B - good balance of size and capability\n",
    "# Alternative: \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"This model will learn to reason step-by-step using GRPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters configured for reasoning training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Reasoning Dataset\n",
    "\n",
    "We'll use GSM8K (Grade School Math 8K) - a dataset of math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset):,} problems\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(f\"Question: {dataset[0]['question']}\")\n",
    "print(f\"Answer: {dataset[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Format Dataset for GRPO\n",
    "\n",
    "GRPO needs:\n",
    "- The problem/question\n",
    "- The correct answer (for reward calculation)\n",
    "- A prompt format that encourages step-by-step reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(answer_text):\n",
    "    \"\"\"\n",
    "    Extract the final numerical answer from GSM8K format.\n",
    "    GSM8K answers are in format: \"Step 1\\nStep 2\\n#### 42\"\n",
    "    \"\"\"\n",
    "    match = re.search(r'####\\s*([\\d,]+)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return None\n",
    "\n",
    "def format_for_grpo(examples):\n",
    "    \"\"\"\n",
    "    Format examples for GRPO training.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    \n",
    "    for question, answer_text in zip(examples['question'], examples['answer']):\n",
    "        # Create prompt that encourages reasoning\n",
    "        prompt = f\"\"\"Solve this math problem step by step. Show your reasoning.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Solution:\"\"\"\n",
    "        \n",
    "        # Extract the numerical answer\n",
    "        answer = extract_answer(answer_text)\n",
    "        \n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "    \n",
    "    return {\"prompt\": prompts, \"answer\": answers}\n",
    "\n",
    "# Format the dataset\n",
    "formatted_dataset = dataset.map(\n",
    "    format_for_grpo,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Use a subset for faster training (remove .select() to use full dataset)\n",
    "train_dataset = formatted_dataset.select(range(2000))\n",
    "\n",
    "print(f\"Formatted {len(train_dataset)} examples for GRPO training\")\n",
    "print(\"\\nExample prompt:\")\n",
    "print(train_dataset[0]['prompt'])\n",
    "print(f\"\\nCorrect answer: {train_dataset[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define Reward Function\n",
    "\n",
    "The reward function checks if the model's answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_answer(text):\n",
    "    \"\"\"\n",
    "    Extract numerical answer from model's response.\n",
    "    Looks for patterns like 'The answer is 42' or 'Answer: 42'\n",
    "    \"\"\"\n",
    "    # Try various patterns\n",
    "    patterns = [\n",
    "        r'[Tt]he answer is\\s*([\\d,]+)',\n",
    "        r'[Aa]nswer:\\s*([\\d,]+)',\n",
    "        r'=\\s*([\\d,]+)\\s*$',\n",
    "        r'####\\s*([\\d,]+)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '')\n",
    "    \n",
    "    # If no pattern matches, try to find last number\n",
    "    numbers = re.findall(r'([\\d,]+)', text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def compute_reward(completions, correct_answers):\n",
    "    \"\"\"\n",
    "    Compute rewards for model completions.\n",
    "    Returns +1 for correct answer, -1 for incorrect.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for completion, correct_answer in zip(completions, correct_answers):\n",
    "        model_answer = extract_model_answer(completion)\n",
    "        \n",
    "        if model_answer is not None and model_answer == correct_answer:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(-1.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"Reward function defined!\")\n",
    "print(\"Correct answers get +1.0, incorrect get -1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Model BEFORE GRPO Training\n",
    "\n",
    "Let's see how well the base model can solve math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_problems = [\n",
    "    \"If John has 5 apples and buys 3 more, how many apples does he have?\",\n",
    "    \"A rectangle is 8 feet long and 5 feet wide. What is its area?\",\n",
    "    \"If a car travels 60 miles per hour for 3 hours, how far does it go?\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL PERFORMANCE BEFORE GRPO TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for problem in test_problems:\n",
    "    prompt = f\"\"\"Solve this math problem step by step. Show your reasoning.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Solution:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nProblem: {problem}\")\n",
    "    print(\"-\"*70)\n",
    "    print(response)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Re-enable training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_config = GRPOConfig(\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    warmup_ratio = 0.1,\n",
    "    num_train_epochs = 1,\n",
    "    max_steps = 500,\n",
    "    learning_rate = 5e-6,  # Lower LR for GRPO\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 10,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs/gemma2_grpo_reasoning\",\n",
    "    report_to = \"none\",\n",
    "    # GRPO-specific settings\n",
    "    num_generations_per_prompt = 4,  # Generate 4 solutions per problem\n",
    "    max_new_tokens = 512,\n",
    "    temperature = 0.9,  # Higher temp for exploration\n",
    ")\n",
    "\n",
    "print(\"GRPO Configuration:\")\n",
    "print(f\"- Generations per prompt: {grpo_config.num_generations_per_prompt}\")\n",
    "print(f\"- Learning rate: {grpo_config.learning_rate}\")\n",
    "print(f\"- Max steps: {grpo_config.max_steps}\")\n",
    "print(\"\\nGRPO will generate multiple solutions and learn from relative ranking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    args = grpo_config,\n",
    "    train_dataset = train_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    reward_function = compute_reward,\n",
    ")\n",
    "\n",
    "print(\"GRPO Trainer created!\")\n",
    "print(\"\\nHow GRPO works:\")\n",
    "print(\"1. For each problem, generate 4 different solutions\")\n",
    "print(\"2. Check which solutions are correct (reward function)\")\n",
    "print(\"3. Rank solutions within each group\")\n",
    "print(\"4. Update model to favor higher-ranked (correct) solutions\")\n",
    "print(\"5. Repeat for all training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Train with GRPO\n",
    "\n",
    "This will take longer than standard fine-tuning because we generate multiple solutions per problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING GRPO TRAINING\")\n",
    "print(\"Training reasoning capabilities...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer_stats = grpo_trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GRPO TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Final loss: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test Model AFTER GRPO Training\n",
    "\n",
    "Let's see if reasoning improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL PERFORMANCE AFTER GRPO TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for problem in test_problems:\n",
    "    prompt = f\"\"\"Solve this math problem step by step. Show your reasoning.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Solution:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nProblem: {problem}\")\n",
    "    print(\"-\"*70)\n",
    "    print(response)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"\\nNotice how the model now:\")\n",
    "print(\"- Shows clearer step-by-step reasoning\")\n",
    "print(\"- Provides more accurate answers\")\n",
    "print(\"- Uses better mathematical notation\")\n",
    "print(\"- Explains its thought process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Test on New Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on harder problems from the dataset\n",
    "test_dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\").select(range(10))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Testing on 10 held-out problems...\\n\")\n",
    "\n",
    "for example in test_dataset:\n",
    "    question = example['question']\n",
    "    correct_answer = extract_answer(example['answer'])\n",
    "    \n",
    "    prompt = f\"\"\"Solve this math problem step by step. Show your reasoning.\n",
    "\n",
    "Problem: {question}\n",
    "\n",
    "Solution:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.3,  # Lower temp for evaluation\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    model_answer = extract_model_answer(response)\n",
    "    \n",
    "    is_correct = model_answer == correct_answer\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    status = \"✓\" if is_correct else \"✗\"\n",
    "    print(f\"{status} Problem {total}: {question[:60]}...\")\n",
    "    print(f\"  Correct: {correct_answer}, Model: {model_answer}\")\n",
    "    print()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"gemma2_grpo_reasoning_adapters\")\n",
    "tokenizer.save_pretrained(\"gemma2_grpo_reasoning_adapters\")\n",
    "\n",
    "print(\"Model saved!\")\n",
    "print(\"\\nThis model can now:\")\n",
    "print(\"- Solve math word problems\")\n",
    "print(\"- Show step-by-step reasoning\")\n",
    "print(\"- Explain mathematical concepts\")\n",
    "print(\"- Break down complex problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Merge and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights\n",
    "model_merged = model.merge_and_unload()\n",
    "model_merged.save_pretrained(\"gemma2_grpo_reasoning_merged\")\n",
    "tokenizer.save_pretrained(\"gemma2_grpo_reasoning_merged\")\n",
    "\n",
    "# Export to GGUF\n",
    "model_merged.save_pretrained_gguf(\n",
    "    \"gemma2_grpo_reasoning_gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method = \"q4_k_m\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model merged and exported to GGUF format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "1. Trained a reasoning model using **GRPO**\n",
    "2. Used GSM8K math problems dataset\n",
    "3. Implemented reward function for correctness\n",
    "4. Generated multiple solutions per problem\n",
    "5. Improved model's step-by-step reasoning\n",
    "\n",
    "### Key Differences from Other Methods:\n",
    "\n",
    "**vs DPO/ORPO:**\n",
    "- GRPO generates multiple solutions and ranks them\n",
    "- Better for problems with verifiable correctness\n",
    "- Uses group-relative ranking instead of pairwise comparison\n",
    "\n",
    "**vs Standard Fine-Tuning:**\n",
    "- GRPO learns from trial and error\n",
    "- Encourages exploration of solution strategies\n",
    "- Better for reasoning and problem-solving tasks\n",
    "\n",
    "### When to use GRPO:\n",
    "- Math problems (GSM8K, MATH dataset)\n",
    "- Code generation with test cases\n",
    "- Logical reasoning tasks\n",
    "- Any problem with verifiable correctness\n",
    "\n",
    "### Performance Expectations:\n",
    "- Base model: ~10-20% accuracy on GSM8K\n",
    "- After GRPO: ~30-50% accuracy (with more training: 60-80%)\n",
    "- For better results: train longer, use more data, larger model\n",
    "\n",
    "### Next Steps:\n",
    "- Try with larger models (Llama 3.1 8B)\n",
    "- Train on full GSM8K dataset\n",
    "- Experiment with different reward functions\n",
    "- Test on MATH dataset (harder problems)\n",
    "- Combine with other datasets (code, logic puzzles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
