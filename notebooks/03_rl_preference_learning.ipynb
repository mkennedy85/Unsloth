{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 3: Reinforcement Learning with Preference Data (DPO/ORPO)\n\nThis notebook demonstrates **Direct Preference Optimization (DPO)** and **Odds Ratio Preference Optimization (ORPO)** using Unsloth.ai.\n\n## Key Concepts\n\n### What is RLHF?\n**Reinforcement Learning from Human Feedback (RLHF)** aligns models with human preferences:\n1. Supervised fine-tuning (SFT) - base instruction following\n2. Reward modeling - learn what humans prefer\n3. RL training - optimize for preferred outputs\n\n### DPO: Simpler than PPO\n**Traditional RLHF** uses PPO (Proximal Policy Optimization) which:\n- Requires separate reward model\n- Complex training loop\n- Unstable training\n\n**DPO** (Direct Preference Optimization):\n- No reward model needed!\n- Directly optimizes on preference pairs\n- More stable and simpler\n\n### ORPO: Even Better\n**ORPO** (Odds Ratio Preference Optimization):\n- Combines SFT + preference learning in one step\n- No separate SFT stage needed\n- Often outperforms DPO\n\n## Dataset Format\n\nPreference datasets need **chosen** (good) and **rejected** (bad) responses:\n\n```json\n{\n  \"prompt\": \"Explain quantum computing\",\n  \"chosen\": \"Quantum computing uses qubits that can be in superposition...\",\n  \"rejected\": \"Quantum is like really fast computers.\"\n}\n```\n\n## Video Recording Checklist\n- [ ] Explain RLHF, DPO, and ORPO differences\n- [ ] Show preference dataset format\n- [ ] Demonstrate why we need preference learning\n- [ ] Compare outputs: base model → SFT → DPO/ORPO\n- [ ] Explain training dynamics\n- [ ] Show alignment improvements"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth and DPO Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture\n# Install Unsloth and dependencies\n# Use colab-new for Google Colab, cu121-torch230 for Vertex AI Workbench\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchDPOTrainer\n",
    "PatchDPOTrainer()  # Apply Unsloth optimizations to DPO trainer\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, ORPOTrainer, ORPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Using DPO/ORPO trainers from TRL library\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Load Pre-trained or SFT Model\n\n**Important**: For best results, start with a model that's already instruction-tuned (SFT).\nWe can use:\n1. Pre-trained instruction model (Llama-3-8B-Instruct)\n2. Model we fine-tuned in Notebook 1 or 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load instruction-tuned model\n",
    "# For demo: using Gemma-2-2B-it (instruction-tuned)\n",
    "# Alternative: \"unsloth/Llama-3.1-8B-Instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"This is an INSTRUCTION-TUNED model (already did SFT)\")\n",
    "print(f\"Now we'll align it with human preferences using DPO/ORPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add LoRA Adapters\n",
    "\n",
    "We'll use LoRA for efficient preference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters added for preference learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Preference Dataset\n",
    "\n",
    "We'll use the Anthropic HH-RLHF dataset - the gold standard for preference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Anthropic's Helpful and Harmless RLHF dataset\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset):,} examples\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(dataset[0])\n",
    "print(\"\\nDataset columns:\", dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Format Dataset for DPO/ORPO\n",
    "\n",
    "The dataset needs specific format:\n",
    "- `prompt`: The user query\n",
    "- `chosen`: Preferred response\n",
    "- `rejected`: Dispreferred response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_preference_data(examples):\n",
    "    \"\"\"\n",
    "    Anthropic HH-RLHF format:\n",
    "    - 'chosen': Full conversation with good response\n",
    "    - 'rejected': Full conversation with bad response\n",
    "    \n",
    "    We need to extract:\n",
    "    - prompt: The user's last message\n",
    "    - chosen: Assistant's good response\n",
    "    - rejected: Assistant's bad response\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    \n",
    "    for chosen, rejected in zip(examples['chosen'], examples['rejected']):\n",
    "        # Split conversations into turns\n",
    "        # Format: \"\\n\\nHuman: ... \\n\\nAssistant: ...\"\n",
    "        \n",
    "        # Extract prompt (everything before the last Assistant response)\n",
    "        prompt_end = chosen.rfind(\"\\n\\nAssistant:\")\n",
    "        prompt = chosen[:prompt_end].strip()\n",
    "        \n",
    "        # Extract chosen response (after last Assistant:)\n",
    "        chosen_response = chosen[prompt_end:].replace(\"\\n\\nAssistant:\", \"\").strip()\n",
    "        \n",
    "        # Extract rejected response\n",
    "        rejected_prompt_end = rejected.rfind(\"\\n\\nAssistant:\")\n",
    "        rejected_response = rejected[rejected_prompt_end:].replace(\"\\n\\nAssistant:\", \"\").strip()\n",
    "        \n",
    "        formatted.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": chosen_response,\n",
    "            \"rejected\": rejected_response\n",
    "        })\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Format first 10000 examples for faster training (use more for better results)\n",
    "dataset_small = dataset.select(range(10000))\n",
    "formatted_data = format_preference_data(dataset_small)\n",
    "\n",
    "# Convert to HF dataset\n",
    "from datasets import Dataset\n",
    "preference_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(f\"Formatted {len(preference_dataset)} preference pairs\")\n",
    "print(\"\\nExample formatted data:\")\n",
    "print(f\"Prompt: {preference_dataset[0]['prompt'][:200]}...\")\n",
    "print(f\"\\nChosen: {preference_dataset[0]['chosen'][:200]}...\")\n",
    "print(f\"\\nRejected: {preference_dataset[0]['rejected'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure DPO Training\n",
    "\n",
    "Let's start with DPO (Direct Preference Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO-specific arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 2,  # DPO needs more memory\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_ratio = 0.1,\n",
    "    num_train_epochs = 1,\n",
    "    max_steps = 500,\n",
    "    learning_rate = 5e-5,  # Lower LR for preference learning\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 10,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs/gemma2_dpo\",\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "print(\"DPO training configuration:\")\n",
    "print(f\"Beta (KL penalty): 0.1 (default, controls how much model can change)\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"This trains the model to prefer 'chosen' over 'rejected' responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create DPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,  # Unsloth handles reference model internally\n",
    "    args = training_args,\n",
    "    beta = 0.1,  # KL divergence penalty (how much model can deviate from reference)\n",
    "    train_dataset = preference_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 512,  # Max sequence length\n",
    "    max_prompt_length = 256,  # Max prompt length\n",
    ")\n",
    "\n",
    "print(\"DPO Trainer created!\")\n",
    "print(\"\\nHow DPO works:\")\n",
    "print(\"1. Shows model a prompt + chosen response\")\n",
    "print(\"2. Shows model same prompt + rejected response\")\n",
    "print(\"3. Trains model to assign higher probability to chosen\")\n",
    "print(\"4. Uses reference model to prevent too much deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Model BEFORE DPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompt = \"What's the best way to learn programming?\"\n",
    "\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "response_before = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL OUTPUT BEFORE DPO TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(response_before)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Re-enable training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Train with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING DPO TRAINING\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "dpo_stats = dpo_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DPO TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training time: {dpo_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Loss: {dpo_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Test Model AFTER DPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test with same prompt\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "response_after = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL OUTPUT AFTER DPO TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(response_after)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"BEFORE DPO:\")\n",
    "print(response_before)\n",
    "print(\"\\nAFTER DPO:\")\n",
    "print(response_after)\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNotice how the response is more:\")\n",
    "print(\"- Helpful and detailed\")\n",
    "print(\"- Harmless (avoids problematic content)\")\n",
    "print(\"- Aligned with human preferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Try ORPO Instead (Optional)\n",
    "\n",
    "ORPO often works better than DPO and doesn't need a reference model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fresh model for ORPO\n",
    "model_orpo, tokenizer_orpo = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model_orpo = FastLanguageModel.get_peft_model(\n",
    "    model_orpo,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"Fresh model loaded for ORPO comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORPO Configuration\n",
    "orpo_config = ORPOConfig(\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_ratio = 0.1,\n",
    "    num_train_epochs = 1,\n",
    "    max_steps = 500,\n",
    "    learning_rate = 8e-6,  # ORPO uses lower LR\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 10,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs/gemma2_orpo\",\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model = model_orpo,\n",
    "    args = orpo_config,\n",
    "    train_dataset = preference_dataset,\n",
    "    tokenizer = tokenizer_orpo,\n",
    "    max_length = 512,\n",
    "    max_prompt_length = 256,\n",
    ")\n",
    "\n",
    "print(\"ORPO Trainer created!\")\n",
    "print(\"\\nORPO advantages over DPO:\")\n",
    "print(\"- No reference model needed (saves memory)\")\n",
    "print(\"- Combines SFT + preference learning\")\n",
    "print(\"- Often more stable training\")\n",
    "print(\"- Better final performance in many cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING ORPO TRAINING\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "orpo_stats = orpo_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ORPO TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training time: {orpo_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Loss: {orpo_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Compare DPO vs ORPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model_orpo)\n",
    "\n",
    "# Test ORPO model\n",
    "inputs = tokenizer_orpo([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model_orpo.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
    "response_orpo = tokenizer_orpo.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: DPO vs ORPO\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nBase Model:\")\n",
    "print(response_before)\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"\\nAfter DPO:\")\n",
    "print(response_after)\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"\\nAfter ORPO:\")\n",
    "print(response_orpo)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DPO model\n",
    "model.save_pretrained(\"gemma2_dpo_aligned\")\n",
    "tokenizer.save_pretrained(\"gemma2_dpo_aligned\")\n",
    "\n",
    "# Save ORPO model\n",
    "model_orpo.save_pretrained(\"gemma2_orpo_aligned\")\n",
    "tokenizer_orpo.save_pretrained(\"gemma2_orpo_aligned\")\n",
    "\n",
    "print(\"Both models saved!\")\n",
    "print(\"DPO model: gemma2_dpo_aligned/\")\n",
    "print(\"ORPO model: gemma2_orpo_aligned/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Export to GGUF/Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and export DPO model\n",
    "model_merged = model.merge_and_unload()\n",
    "model_merged.save_pretrained_gguf(\n",
    "    \"gemma2_dpo_gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method = \"q4_k_m\"\n",
    ")\n",
    "\n",
    "# Merge and export ORPO model\n",
    "model_orpo_merged = model_orpo.merge_and_unload()\n",
    "model_orpo_merged.save_pretrained_gguf(\n",
    "    \"gemma2_orpo_gguf\",\n",
    "    tokenizer_orpo,\n",
    "    quantization_method = \"q4_k_m\"\n",
    ")\n",
    "\n",
    "print(\"Models exported to GGUF format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "1. Learned about RLHF, DPO, and ORPO\n",
    "2. Trained models using **preference data** (chosen vs rejected)\n",
    "3. Compared DPO and ORPO approaches\n",
    "4. Demonstrated alignment improvements\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "**Traditional RLHF (PPO)**:\n",
    "- ❌ Complex: Needs separate reward model\n",
    "- ❌ Unstable: Training can diverge\n",
    "- ❌ Slow: Multiple models to train\n",
    "\n",
    "**DPO (Direct Preference Optimization)**:\n",
    "- ✅ Simple: No reward model\n",
    "- ✅ Stable: Direct optimization\n",
    "- ⚠️ Needs reference model (more memory)\n",
    "\n",
    "**ORPO (Odds Ratio PO)**:\n",
    "- ✅ No reference model (saves memory)\n",
    "- ✅ Combines SFT + preference learning\n",
    "- ✅ Often best performance\n",
    "\n",
    "### When to use preference learning:\n",
    "- Aligning model with human values\n",
    "- Improving response quality\n",
    "- Teaching helpfulness/harmlessness\n",
    "- Reducing unwanted behaviors\n",
    "\n",
    "### Dataset Requirements:\n",
    "- Pairs of (prompt, chosen, rejected)\n",
    "- High-quality preference labels\n",
    "- Diverse examples\n",
    "- 10k+ pairs recommended\n",
    "\n",
    "### Next Steps:\n",
    "- Try with your own preference data\n",
    "- Experiment with different beta values (DPO)\n",
    "- Test on larger models (Llama 3.1 8B)\n",
    "- Combine with reinforcement learning (GRPO)\n",
    "\n",
    "### Performance Tips:\n",
    "1. Start with instruction-tuned model (not base model)\n",
    "2. Use high-quality preference data\n",
    "3. Lower learning rate than SFT (5e-6 to 5e-5)\n",
    "4. ORPO often better than DPO\n",
    "5. More preference pairs = better alignment"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}