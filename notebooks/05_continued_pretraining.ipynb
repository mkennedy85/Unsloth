{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Continued Pretraining for New Language\n",
    "\n",
    "This notebook demonstrates **continued pretraining** to teach an LLM a new language or domain-specific knowledge.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What is Continued Pretraining?\n",
    "**Continued pretraining** continues the original pretraining process on new data:\n",
    "- Extends model's knowledge to new languages/domains\n",
    "- Uses next-token prediction (like original pretraining)\n",
    "- Different from fine-tuning (which uses instruction/response pairs)\n",
    "- Requires large amounts of text data\n",
    "\n",
    "### Pretraining vs Fine-Tuning\n",
    "\n",
    "**Pretraining:**\n",
    "- Learns language patterns from raw text\n",
    "- Predicts next token: \"The cat sat on the ___\" → \"mat\"\n",
    "- Needs millions/billions of tokens\n",
    "- Builds foundational understanding\n",
    "\n",
    "**Fine-Tuning:**\n",
    "- Teaches specific tasks (chat, QA, etc.)\n",
    "- Uses structured instruction/response pairs\n",
    "- Needs thousands of examples\n",
    "- Adapts existing knowledge\n",
    "\n",
    "### When to Use Continued Pretraining?\n",
    "- Teaching a new language (e.g., Spanish, Japanese)\n",
    "- Domain adaptation (medical, legal, scientific text)\n",
    "- Updating with new knowledge (recent events, new terminology)\n",
    "- Low-resource languages\n",
    "\n",
    "## Dataset Format\n",
    "\n",
    "Just raw text in the target language/domain:\n",
    "\n",
    "```\n",
    "El sol brilla en el cielo. Los pájaros cantan en los árboles.\n",
    "La gente camina por las calles de la ciudad.\n",
    "```\n",
    "\n",
    "## Video Recording Checklist\n",
    "- [ ] Explain difference between pretraining and fine-tuning\n",
    "- [ ] Show dataset preparation (monolingual corpus)\n",
    "- [ ] Demonstrate perplexity evaluation\n",
    "- [ ] Test model's language understanding before/after\n",
    "- [ ] Show bilingual capabilities\n",
    "- [ ] Discuss when to use this technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Unsloth and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth and dependencies\n",
    "# Use colab-new for Google Colab, cu121-torch230 for Vertex AI Workbench\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from unsloth import is_bfloat16_supported\n",
    "import math\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Base Model\n",
    "\n",
    "We'll use a multilingual-capable model as our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Using SmolLM2 as base - it has some multilingual capability\n",
    "# Alternative: \"unsloth/Llama-3.2-3B\" or \"unsloth/Qwen2.5-1.5B\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/SmolLM2-360M\",  # Slightly larger for better learning\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(\"\\nWe'll teach this model to better understand Spanish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Model's Spanish BEFORE Training\n",
    "\n",
    "Let's see how well the base model understands Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "spanish_prompts = [\n",
    "    \"El sol brilla en\",\n",
    "    \"Los niños juegan en\",\n",
    "    \"Me gusta comer\",\n",
    "    \"Buenos días, ¿cómo\",\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL'S SPANISH UNDERSTANDING BEFORE CONTINUED PRETRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for prompt in spanish_prompts:\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Model: {completion}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\nNotice: The model may struggle with Spanish or switch to English\\n\")\n",
    "\n",
    "# Back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add LoRA for Continued Pretraining\n",
    "\n",
    "Even for pretraining, we can use LoRA for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,  # Higher rank for pretraining\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.0,  # No dropout for pretraining\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters configured for continued pretraining!\")\n",
    "print(\"Note: Higher rank (r=32) for better language learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Spanish Text Dataset\n",
    "\n",
    "We'll use Spanish Wikipedia or news articles for pretraining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spanish Wikipedia dataset\n",
    "# Alternative: \"MC4\" (Spanish), \"oscar-corpus/OSCAR-2301\" (Spanish)\n",
    "dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.es\",  # Spanish Wikipedia\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset):,} articles\")\n",
    "print(\"\\nFirst article (truncated):\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prepare Dataset for Pretraining\n",
    "\n",
    "For pretraining, we just need raw text tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text for causal language modeling.\n",
    "    No special formatting needed - just raw text.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=False,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Use a subset for training (for demo purposes)\n",
    "# For real continued pretraining, use much more data!\n",
    "train_dataset = dataset.select(range(10000))  # 10K articles\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_dataset)} examples\")\n",
    "print(f\"Average tokens per example: {sum(len(x['input_ids']) for x in tokenized_dataset) / len(tokenized_dataset):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Data Collator for Language Modeling\n",
    "\n",
    "This handles creating labels for next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"Data collator created for causal language modeling\")\n",
    "print(\"This will automatically create labels for next-token prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Configure Training for Continued Pretraining\n",
    "\n",
    "Pretraining uses different hyperparameters than fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 100,\n",
    "    num_train_epochs = 1,\n",
    "    max_steps = 1000,  # For demo; use more for real pretraining\n",
    "    learning_rate = 3e-4,  # Higher LR for pretraining\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    logging_steps = 50,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    seed = 3407,\n",
    "    output_dir = \"outputs/smollm2_spanish_pretrain\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = 500,\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Learning rate: {training_args.learning_rate} (higher than fine-tuning!)\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total steps: {training_args.max_steps}\")\n",
    "print(\"\\nNote: Real continued pretraining needs millions of steps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create Trainer and Start Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset,\n",
    "    data_collator = data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer created for continued pretraining!\")\n",
    "print(\"\\nWhat happens during pretraining:\")\n",
    "print(\"1. Model sees Spanish text\")\n",
    "print(\"2. Tries to predict next token\")\n",
    "print(\"3. Gets feedback on prediction accuracy\")\n",
    "print(\"4. Updates weights to improve Spanish understanding\")\n",
    "print(\"5. Repeats for all text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Train the Model\n",
    "\n",
    "This will teach the model Spanish patterns and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING CONTINUED PRETRAINING\")\n",
    "print(\"Teaching Spanish to the model...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONTINUED PRETRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"Perplexity: {math.exp(trainer_stats.metrics['train_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test Model's Spanish AFTER Training\n",
    "\n",
    "Let's see if Spanish improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL'S SPANISH UNDERSTANDING AFTER CONTINUED PRETRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for prompt in spanish_prompts:\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Model: {completion}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\nNotice: The model should now:\")\n",
    "print(\"- Generate more fluent Spanish\")\n",
    "print(\"- Use correct grammar and vocabulary\")\n",
    "print(\"- Stay in Spanish (not switch to English)\")\n",
    "print(\"- Complete sentences naturally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Test Bilingual Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_tests = [\n",
    "    (\"English\", \"The weather today is\"),\n",
    "    (\"Spanish\", \"El clima de hoy es\"),\n",
    "    (\"English\", \"I like to eat\"),\n",
    "    (\"Spanish\", \"Me gusta comer\"),\n",
    "    (\"English\", \"The capital of France is\"),\n",
    "    (\"Spanish\", \"La capital de Francia es\"),\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BILINGUAL CAPABILITIES TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for lang, prompt in bilingual_tests:\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\n[{lang}] {prompt}\")\n",
    "    print(f\"→ {completion}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"The model should maintain both English AND Spanish!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Evaluate Perplexity\n",
    "\n",
    "Perplexity measures how \"surprised\" the model is by the text. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "test_dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.es\",\n",
    "    split=\"train[:100]\",  # Small test set\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Tokenize test set\n",
    "test_tokenized = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate(test_tokenized)\n",
    "\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERPLEXITY EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "print(\"\\nLower perplexity = better language understanding\")\n",
    "print(\"Typical values: 10-50 (good), 50-100 (okay), >100 (needs more training)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"smollm2_spanish_adapters\")\n",
    "tokenizer.save_pretrained(\"smollm2_spanish_adapters\")\n",
    "\n",
    "print(\"Model saved!\")\n",
    "print(\"\\nThis model now has:\")\n",
    "print(\"- Improved Spanish understanding\")\n",
    "print(\"- Better Spanish generation\")\n",
    "print(\"- Maintained English capability\")\n",
    "print(\"- Bilingual abilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Merge and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights\n",
    "model_merged = model.merge_and_unload()\n",
    "model_merged.save_pretrained(\"smollm2_spanish_merged\")\n",
    "tokenizer.save_pretrained(\"smollm2_spanish_merged\")\n",
    "\n",
    "# Export to GGUF\n",
    "model_merged.save_pretrained_gguf(\n",
    "    \"smollm2_spanish_gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method = \"q4_k_m\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model merged and exported to GGUF format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "1. Taught an English-centric model to understand Spanish\n",
    "2. Used **continued pretraining** on Spanish Wikipedia\n",
    "3. Maintained bilingual capabilities\n",
    "4. Evaluated perplexity improvements\n",
    "5. Created a Spanish-capable model\n",
    "\n",
    "### Key Differences from Fine-Tuning:\n",
    "\n",
    "**Continued Pretraining:**\n",
    "- Raw text, no structure\n",
    "- Next-token prediction objective\n",
    "- Higher learning rate (3e-4 vs 5e-5)\n",
    "- Needs millions of tokens\n",
    "- Builds foundational knowledge\n",
    "- Use rank=32 or higher for LoRA\n",
    "\n",
    "**Fine-Tuning:**\n",
    "- Structured instruction/response pairs\n",
    "- Teaches specific behaviors\n",
    "- Lower learning rate\n",
    "- Needs thousands of examples\n",
    "- Adapts existing knowledge\n",
    "- Use rank=8-16 for LoRA\n",
    "\n",
    "### When to Use Continued Pretraining:\n",
    "\n",
    "**Good Use Cases:**\n",
    "- Teaching new languages\n",
    "- Domain adaptation (medical, legal)\n",
    "- Adding new knowledge (recent events)\n",
    "- Low-resource language support\n",
    "- Extending vocabulary\n",
    "\n",
    "**Not Recommended For:**\n",
    "- Teaching specific tasks → use fine-tuning\n",
    "- Small datasets → use few-shot learning\n",
    "- Time-sensitive projects → training takes long\n",
    "\n",
    "### Data Requirements:\n",
    "- **Minimum**: 100M tokens (~500K articles)\n",
    "- **Good**: 1B+ tokens (millions of articles)\n",
    "- **Production**: 10B+ tokens\n",
    "- Quality > Quantity (clean, well-written text)\n",
    "\n",
    "### Training Tips:\n",
    "1. Use larger batch sizes (16-32)\n",
    "2. Train for multiple epochs\n",
    "3. Monitor perplexity on validation set\n",
    "4. Use cosine learning rate schedule\n",
    "5. Save checkpoints regularly\n",
    "6. Test on diverse examples\n",
    "\n",
    "### Expected Results:\n",
    "With proper training:\n",
    "- Perplexity: 15-30 (excellent)\n",
    "- Fluent text generation\n",
    "- Correct grammar and spelling\n",
    "- Natural language patterns\n",
    "\n",
    "### Next Steps:\n",
    "- Try other languages (French, German, Japanese)\n",
    "- Domain-specific pretraining (medical, code)\n",
    "- Combine with instruction fine-tuning\n",
    "- Test on downstream tasks\n",
    "- Create multilingual models\n",
    "\n",
    "### Real-World Applications:\n",
    "- Multilingual chatbots\n",
    "- Translation systems\n",
    "- Cross-lingual search\n",
    "- Cultural content generation\n",
    "- Language preservation (endangered languages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
